---
title: 'First blog post. Coding (naive) SGEMM in CUDA'
date: 08/02/2025
permalink: /posts/naive-sgemm-cuda/
tags:
  - CUDA
  - SGEMM
  - naive
---

## Naive SGEMM implementation
This is the first of hopefully many posts to follow on getting familiar with CUDA. My plan is to document my journey
in this [repository](https://github.com/RafaelPo/cuda_journey). My current plan is to initially spend some time going through Simon Boehm's blog [post](https://siboehm.com/articles/22/CUDA-MMM) on optimising CUDA matmul kernels as well as watching a few more of the [gpu-mode](https://github.com/gpu-mode/lectures/tree/main?tab=readme-ov-file) lectures.

Today I first listened to Jeremy Howard's lecture while going through the same notebook where he explains two algorithms: 1) RGB-to-Greyscale conversion and 2) matrix multiplication. Following this I wanted something slightly different so I implemented SGEMM in google colab. Lastly I modified the code slightly so that I can run it on LeetGPU. 

For reference SGEMM describes the following: 

$$
\mathbf{C} \leftarrow \alpha \cdot \mathbf{AB} + \beta \cdot \mathbf{C}
$$

For the implementation of SGEMM I followed Simon Boehm's naive implementation (see Kernel 1 in his post) where the kernel -which will be executed by a thread- calculates the following: 

$$
\mathbf{C}_{i,j} \leftarrow \alpha \cdot \mathbf{A}_{i,:}^T\mathbf{B}_{:,j} + \beta \cdot \mathbf{C}_{i,j}
$$


## Python code and CUDA code side-by-side

Jeremy Howard's approach of first writing the python code resembling the style it will need to follow when written in CUDA proved very helpful. Sample from his [notebook](https://github.com/gpu-mode/lectures/blob/main/lecture_003/pmpp.ipynb):

```python
def blk_kernel(f, blocks, threads, *args):
    for i in range(blocks):
        for j in range(threads): f(i, j, threads, *args)

def rgb2grey_bk(blockidx, threadidx, blockdim, x, out, n):
    i = blockidx*blockdim + threadidx
    if i<n: out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n]

def rgb2grey_pybk(x):
    c,h,w = x.shape
    n = h*w
    x = x.flatten()
    res = torch.empty(n, dtype=x.dtype, device=x.device)
    threads = 256
    blocks = int(math.ceil(h*w/threads))
    blk_kernel(rgb2grey_bk, blocks, threads, x, res, n)
    return res.view(h,w)
```

```cpp
__global__ void rgb_to_grayscale_kernel(unsigned char* x, unsigned char* out, int n) {
    int i = blockIdx.x*blockDim.x + threadIdx.x;
    if (i<n) out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n];
}

torch::Tensor rgb_to_grayscale(torch::Tensor input) {
    CHECK_INPUT(input);
    int h = input.size(1);
    int w = input.size(2);
    printf("h*w: %d*%d\n", h, w);
    auto output = torch::empty({h,w}, input.options());
    int threads = 256;
    rgb_to_grayscale_kernel<<<cdiv(w*h,threads), threads>>>(
        input.data_ptr<unsigned char>(), output.data_ptr<unsigned char>(), w*h);
    C10_CUDA_KERNEL_LAUNCH_CHECK();
    return output;
```


------


## SGEMM code explained

```cpp

__global__ void sgemm_kernel(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {
    // Part 1
    int row = blockIdx.y * blockDim.y + threadIdx.y; 
    int col = blockIdx.x * blockDim.x + threadIdx.x; 

    // Part 2
    if (row >= dim || col >= dim) return; 
    
    // Part 3
    float tmp = 0;
    for (int i = 0; i < dim; ++i) { 
        tmp += (matrix_a[row * dim + i] * matrix_b[i * dim + col]);
    }
    
    // Part 4
    matrix_c[row * dim + col] = alpha * tmp + beta * matrix_c[row * dim + col];
}

```

**Part 1:**
This is the kernel which will be executed by a single "thread call". 
- `blockIdx` -> tuple of ints that locates a block in the grid
- `blockDim` -> tuple of ints describing the shape of the block
- `threadIdx` -> tuple of ints that locates the block within the block

We first use all three to get the "global" 2D location for this execution. All threads will execute the same kernel but with different inputs for `blockIdx`, `blockDim` and `threadIdx`.

**Part 2:**
Ensuring we stay within the bounds of the data.

**Part 3:**
Vector product between A's row and B's column

**Part 4:**
Putting it all together.

